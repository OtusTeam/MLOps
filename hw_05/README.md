# Периодический запуск процедуры очистки датасета мошеннических финансовых транзакций

**Домашнее задание №5**  
**Курс MLOps**  
**Образовательная платформа «Otus»**

## Цель работы

В данном домашнем задании Вы потренируетесь в создании инфраструктуры с помощью инструмента Apache Airflow, организации периодического запуска процедуры очистки данных, познакомитесь с концепцией ориентированных направленных графов (DAGs), с помощью которых организуется последовательность запуска задач по расписанию, научитесь разрабатывать собственные графы с помощью языка Python для Apache Airflow.

## Описание проекта

> Итак, данные очищены и загружены в хранилище. В принципе, можно бы уже начинать их анализ, однако, Вас беспокоит одна проблема. Антифрод-система продолжает работать и накапливать данные, а очистку этих данных Вы выполнили лишь единожды.
> 
> Мошенники продолжают искать новые уязвимые места в системе защиты, а это означает, что модель, обученная на уже собранных и не обновляемых данных, скоро устареет и будет неспособна к качественному анализу транзакций...
> 
> Из сказанного выше вытекает необходимость создания системы, способной периодически получать новые данные из озера компании, проверять их качество, очищать и добавлять к уже существующим в Вашем хранилище. Скрипт для очистки датасета у Вас уже есть; теперь нужно с помощью Apache Airflow обеспечить его периодический запуск на требуемой порции новых данных.

## Требования к инфраструктуре

### Облачная среда

Систему Apache Airflow желательно запустить в облачной среде (с использованием terraform), что позволит динамически создавать необходимые ресурсы и уничтожать их после выполнения задания для экономии ресурсов.

## Задания

Решение будет ожидаться в виде репозитория/ветки на GitHub, с terraform конфигурациями и другим необходимым кодом для запуска всей системы.

### Обязательные задания

1. **Запустить систему Apache Airflow** в сервисе Yandex Cloud Managed Service for Apache Airflow.

2. **Создать DAG для ежедневного автоматизированного создания и удаления** Spark-кластера и запуска скрипта очистки датасета и разместить его в директории для DAG'ов, доступной Apache Airflow. Для этого можно удобно использовать S3 bucket. В графе следует прописать этапы копирования скрипта и необходимых ему файлов на Spark-кластер, а также его запуска на кластере посредством spark-submit.

3. **Убедиться, что граф загрузился** в систему и отображается в графическом интерфейсе. Файл(-ы) с DAG необходимо разместить в Вашем GitHub-репозитории и предоставить для проверки.

4. **Разрешить периодическое исполнение** разработанного DAG в Apache AirFlow и протестировать его работоспособность. Требуется дождаться не менее трёх успешных запусков процедуры очистки датасета по расписанию. Снимок экрана, подтверждающий успешную работу системы, необходимо привести в README-файле Вашего GitHub-репозитория.

### Дополнительные задания

5. **Изменить статус задач** на Kanban-доске в GitHub Projects в соответствии с достигнутыми результатами. Возможно, некоторые задачи нужно будет скорректировать, разделить на подзадачи или объединить друг с другом.

6. **Добавить CI/CD пайплайн в GitHub Actions** для автоматизации изменения кода DAG'ов и python-скриптов.

7. **Полностью удалить созданный кластер**, чтобы избежать оплаты ресурсов в период его простаивания.

## Критерии оценки

Для получения положительной оценки за работу необходимо выполнить **минимум первые четыре** вышеприведенных задания.

---

**Желаем успехов!** 