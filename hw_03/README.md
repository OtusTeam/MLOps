# Анализ качества и очистка датасета мошеннических финансовых операций

**Домашнее задание №3**  
**Курс MLOps**  
**Образовательная платформа «Otus»**

## Цель работы

В данном домашнем задании Вы познакомитесь с основными проблемами, которые могут встречаться в данных, потренируетесь определять их наличие в датасете, обрабатывать и очищать набор данных, разрабатывать скрипты очистки данных с использованием Apache Spark.

## Описание проекта

> Вы создали свое объектное хранилище, в котором разместили фрагмент данных о совершенных финансовых транзакциях компании, предоставленный системным администратором. Казалось бы, можно постепенно приступать к их анализу, однако, Вас смущает один момент. Структуру данных в датасете Вы знаете, но вопрос их качества остается открытым. Неясно, что конкретно находится в тех гигабайтах данных, которые Вы получили.
> 
> К тому же, во время очередного обеденного перерыва Вы пообщались с коллегами, занимающимися обслуживанием системы транзакций, и они с улыбкой рассказали Вам, как занимались ее восстановлением после очередного отказа... Скорее всего, сбор данных тоже в эти моменты нарушался, поэтому нужно проанализировать содержимое датасета на отсутствие в нем ошибок.
> 
> Поскольку процесс оценки качества и очистки датасета придется выполнять периодически, необходим скрипт, выполняющий эти действия.

## Требования к инфраструктуре

### Экономия ресурсов

В целях экономии ресурсов в Yandex cloud все манипуляции необходимо произвести с использованием terraform скриптов, которые являются частью домашнего задания и должны быть представлены в вашем GitHub репозитории.

### Кластер Yandex Data Processing

Spark-кластер должен иметь следующие характеристики:

- **Мастер-подкластер**: класс хоста `s3-c2-m8`, размер хранилища 40 ГБ  
- **Data-подкластер**: класс хоста `s3-c4-m16`, от 3 хостов, размер хранилища от 128 ГБ  
**Важное замечание:** Квоты на создание кластера в вашем аккаунте могут быть ограничены, поэтому если не удается создать кластер, нужно запросить у увеличение квот в поддержке Yandex Cloud.

Требуемый объем локальных дисков зависит от объема кэшированных DataFrames в процессе работы Вашего скрипта.

### Сетевые настройки

Для использования кластера в учебных целях проще всего организовать публичный доступ к мастер-узлу. Данное действие потребует создания группы безопасности, ограничивающей возможные соединения. Кроме разрешений входящих и исходящих соединений, указанных в документации, для удобства работы с кластером рекомендуется разрешить:

- входящие TCP-соединения на порт 22 для SSH
- входящие TCP-соединения на порт 8888 для Jupyter Notebook
- исходящие TCP-соединения со всех портов мастер-узла

## Задания

Решение будет ожидаться в виде репозитория/ветки на GitHub, с terraform конфигурациями и другим необходимым кодом для запуска всей системы.

### Обязательные задания

1. **Создать сервисный аккаунт** в Yandex Cloud для работы с кластером Yandex Data Processing и предоставить ему необходимые роли (необходимые роли подробно указаны в официальной документации).

2. **Создать новый bucket** в Yandex Cloud Object Storage и предоставить созданному выше системному аккаунту право на запись к нему. Для проверки преподавателем данный bucket необходимо сделать общедоступным на чтение, а точку доступа к нему привести в README-файле Вашего GitHub-репозитория.

3. **Создать Spark-кластер в Yandex Data Processing**, указав в настройках созданный выше bucket, с двумя подкластерами согласно указанным характеристикам.

4. **Проанализировать датасет** мошеннических транзакций на наличие в нем ошибочных данных. Данное действие рекомендуется выполнять с помощью среды Jupyter Notebook, запущенной на мастер-узле кластера. Нужно оценить, какие из основных проблем с данными могут иметь место в рассматриваемом датасете, и постараться выявить факт их наличия, колонки, которые они затрагивают, объем некорректных данных и т.д.

5. **Создать скрипт очистки данных** на основе проведенного анализа качества с использованием Apache Spark. Скрипт должен иметь возможность автоматического запуска внешней системой.

6. **Выполнить очистку датасета** с использованием созданного скрипта и сохранить его в созданном выше bucket'е в формате parquet, подходящем для хранения большого объема структурированных данных.

### Дополнительные задания

7. **Изменить статус задач** на Kanban-доске в GitHub Projects в соответствии с достигнутыми результатами. Возможно, некоторые задачи нужно будет скорректировать, разделить на подзадачи или объединить друг с другом.

8. **Полностью удалить созданный кластер**, чтобы избежать оплаты ресурсов в период его простаивания.

## Справочная информация

### Проблемы качества данных

Основные проблемы, которые могут встречаться в данных, описаны в статье:
[Data cleansing - Data quality](https://en.wikipedia.org/wiki/Data_cleansing#Data_quality)

## Критерии оценки

Для получения положительной оценки за работу необходимо выполнить **минимум первые шесть** вышеприведенных заданий, **обнаружив не менее трех типов некорректных данных**.

---

**Желаем успехов!** 